{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import random\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text! \n",
    "\n",
    "(To explain this in more detail - you feed in a sequence of length $N$, then sample from the probability distribution over the $N+1$-th word, use this to construct a new sequence of length $N+1$, then feed this new sequence into the model to get a probability distribution over the $N+2$-th word, and so on.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        \"\"\"\n",
    "        Here we will assume that the input dimensions are same as the\n",
    "        output dims.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_layer = torch.nn.Linear(d, d)\n",
    "        self.k_layer = torch.nn.Linear(d, d)\n",
    "        self.v_layer = torch.nn.Linear(d, d)\n",
    "\n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        \"\"\"\n",
    "        Assume x is <t x d> -- t being the sequence length, d\n",
    "        the embed dims.\n",
    "\n",
    "        W_q, W_k, and W_v are weights for projecting into queries,\n",
    "        keys, and values, respectively. Here these will have shape\n",
    "        <d x t>, yielding d dimensional vectors for each input.\n",
    "\n",
    "        This function should return a t dimensional attention vector\n",
    "        for each input -- i.e., an attention matrix with shape <t x t>,\n",
    "        and the values derived from this <t x d>.\n",
    "\n",
    "        Derive Q, K, V matrices, then self attention weights. These should\n",
    "        be used to compute the final representations (t x d); optionally\n",
    "        return the weights matrix if `return_weights=True`.\n",
    "        \"\"\"\n",
    "        Q = self.q_layer(x)\n",
    "        K = self.k_layer(x)\n",
    "        V = self.v_layer(x)\n",
    "\n",
    "        A = Q @ K.transpose(-2, -1)\n",
    "        if mask is not None:\n",
    "            A = A.masked_fill(mask == 0, -1e9)\n",
    "        weights = F.softmax(A, dim=-1)\n",
    "\n",
    "        if return_weights:\n",
    "          return weights, weights @ V\n",
    "\n",
    "        return weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.layers = nn.ModuleList([SelfAttentionLayer(embed_dim) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = torch.triu(torch.ones(\n",
    "            src.size(1), src.size(1), dtype=torch.bool, device=src.device)).T\n",
    "        src = self.embed(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask)\n",
    "        src = self.norm(src)\n",
    "        output = self.decoder(src)\n",
    "        return output\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.self_attn = SingleHeadAttention(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        src = src + self.norm1(self.self_attn(src, mask=mask))\n",
    "        return src\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(100.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
